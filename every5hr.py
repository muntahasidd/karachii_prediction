# -*- coding: utf-8 -*-
"""every5hr.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18CjZmkZYue9LYeVMED2_xLkI6NZPXoRl
"""

import requests, pandas as pd, numpy as np, time, os, sys
from datetime import datetime, timedelta
import hopsworks, joblib
from hsml.schema import Schema
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ======= CONFIG =======
os.environ["HOPSWORKS_API_KEY"] = os.getenv("HOPSWORKS_API_KEY")
OPENWEATHER_API_KEY = os.getenv("OPENWEATHER_API_KEY")
LAT, LON = 24.8607, 67.0011
CITY_ID = 1174872
CSV_PATH = "karachi_weather_5h.csv"

# ======= FETCH FUNCTIONS =======
def fetch_weather_at(ts):
    url = "http://history.openweathermap.org/data/2.5/history/city"
    params = {
        "id": CITY_ID,
        "type": "hour",
        "start": int(ts.timestamp()),
        "end": int((ts + timedelta(hours=1)).timestamp()),
        "appid": API_KEY
    }
    r = requests.get(url, params=params, timeout=30)
    return r.json()["list"][0] if r.status_code == 200 and r.json().get("list") else None

def fetch_pollution_at(ts):
    url = "https://api.openweathermap.org/data/2.5/air_pollution/history"
    params = {
        "lat": LAT,
        "lon": LON,
        "start": int(ts.timestamp()),
        "end": int((ts + timedelta(hours=1)).timestamp()),
        "appid": API_KEY
    }
    r = requests.get(url, params=params, timeout=30)
    return r.json()["list"][0] if r.status_code == 200 and r.json().get("list") else None

def build_record(ts):
    weather = fetch_weather_at(ts)
    pollution = fetch_pollution_at(ts)
    time.sleep(1)
    if not weather or not pollution:
        print(f" Skipped {ts}: incomplete data")
        return None
    comp = pollution["components"]
    return {
        "timestamp": ts.strftime("%Y-%m-%d %H:%M:%S"),
        "aqi": pollution["main"]["aqi"],
        "pm2_5": comp.get("pm2_5"),
        "pm10": comp.get("pm10"),
        "temperature": round(weather["main"]["temp"] - 273.15, 2),
        "humidity": weather["main"]["humidity"],
        "wind_speed": weather["wind"]["speed"]
    }

# ======= COLLECT DATA =======
def collect_data_5days_every5hours():
    start = datetime.now() - timedelta(days=5)
    end = datetime.now()
    current = start
    records = []

    while current < end:
        print(f" Fetching {current}")
        rec = build_record(current)
        if rec:
            records.append(rec)
        current += timedelta(hours=5)

    df_new = pd.DataFrame(records)
    if os.path.exists(CSV_PATH):
        old = pd.read_csv(CSV_PATH)
        df_all = pd.concat([old, df_new], ignore_index=True).drop_duplicates(subset=["timestamp"])
    else:
        df_all = df_new

    df_all.to_csv(CSV_PATH, index=False)
    print(f" Saved {len(df_all)} total records to {CSV_PATH}")
    return df_all

# ======= PREPROCESSING =======
def cap_and_scale(df, cols):
    df = df.copy()
    for col in cols:
        Q1, Q3 = df[col].quantile(0.25), df[col].quantile(0.75)
        IQR = Q3 - Q1
        df[col] = np.clip(df[col], Q1 - 1.5*IQR, Q3 + 1.5*IQR)
    scaler = MinMaxScaler()
    df[cols] = scaler.fit_transform(df[cols])
    return df, scaler

def prepare_multioutput_forecast_data(df, lag=5, horizon=5):
    if len(df) < lag + horizon:
        print(f" Not enough rows. Need at least {lag + horizon}, got {len(df)}.")
        return None, None
    lag_df = pd.concat([df["aqi"].shift(i) for i in range(1, lag + 1)], axis=1)
    lag_df.columns = [f"aqi_lag_{i}" for i in range(1, lag + 1)]
    target_df = pd.concat([df["aqi"].shift(-i) for i in range(1, horizon + 1)], axis=1)
    target_df.columns = [f"target_t_plus_{i}" for i in range(1, horizon + 1)]
    final_df = pd.concat([lag_df, target_df], axis=1).dropna()
    return final_df[lag_df.columns], final_df[target_df.columns]

# ======= MAIN =======
if __name__ == "__main__":
    # 1) Collect data
    df = collect_data_5days_every5hours()

    # 2) Insert into Hopsworks Feature Store
    project = hopsworks.login()
    fs = project.get_feature_store()
    mr = project.get_model_registry()

    df["timestamp"] = pd.to_datetime(df["timestamp"])
    fg = fs.get_or_create_feature_group(
        name="karachi_weather_5h",
        version=1,
        description="5-hour interval weather and AQI for Karachi",
        primary_key=["timestamp"],
        event_time="timestamp"
    )
    fg.insert(df)
    print(" Latest 5-day data inserted into Feature Store")

    # 3) Preprocessing: outlier capping + scaling
    df_processed, scaler = cap_and_scale(df, ["temperature", "humidity", "wind_speed", "pm2_5", "pm10", "aqi"])

    # 4) Prepare features & targets
    X, y = prepare_multioutput_forecast_data(df_processed, lag=5, horizon=5)
    if X is None or y is None:
        print(" Feature generation failed (insufficient data).")
        sys.exit(1)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

    # 5) Train Random Forest
    rf_model = MultiOutputRegressor(RandomForestRegressor(random_state=42))
    rf_model.fit(X_train, y_train)

    # 6) Evaluate
    test_pred = rf_model.predict(X_test)
    rf_mae = mean_absolute_error(y_test, test_pred)
    rf_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
    rf_r2 = r2_score(y_test, test_pred)
    rf_acc = max(0, 1 - (rf_mae / np.mean(y_test)))

    print(" Testing Performance")
    print("MAE:", round(rf_mae, 2), "RMSE:", round(rf_rmse, 2),
          "RÂ²:", round(rf_r2, 4), "Accuracy:", round(rf_acc * 100, 2), "%")

    # 7) Save model to Hopsworks
    joblib.dump(rf_model, "rf_model.pkl", compress=3)
    input_example = X.iloc[0]
    model_schema = Schema(X)

    model = mr.python.create_model(
        name="karachi_aqi_forecaster_5h",
        metrics={
            "accuracy": round(rf_acc, 4),
            "test_mae": round(rf_mae, 2),
            "test_rmse": round(rf_rmse, 2),
            "test_r2": round(rf_r2, 4)
        },
        model_schema=model_schema,
        input_example=input_example,
        description="Random Forest retrained daily with last 5-day, 5-hour interval data"
    )
    saved_model = model.save("rf_model.pkl")
    print(f" Model saved to Hopsworks, accuracy: {round(rf_acc*100, 2)}%")
