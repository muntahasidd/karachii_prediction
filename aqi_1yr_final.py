# -*- coding: utf-8 -*-
"""aqi_1yr_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IWkj-AVWKhWpXYsHCllsDqGyBgxF9NvK
"""

# ========================================
# Karachi Weather & AQI Forecast - 1 Year Data, 5-hour Interval
# ========================================

# ================= IMPORTS =================
import requests
import pandas as pd
from datetime import datetime, timedelta
import time
import os
import numpy as np

# ================= CONFIG =================
API_KEY = "6094f98aa9ad646bfcbdd49788573e5b"  # OpenWeather API key
LAT, LON = 24.8607, 67.0011  # Karachi coordinates
CITY_ID = 1174872
CSV_PATH = "karachi_weather_5h.csv"

# ================= FETCH FUNCTIONS =================
def fetch_weather_at(timestamp):
    url = "http://history.openweathermap.org/data/2.5/history/city"
    params = {
        "id": CITY_ID,
        "type": "hour",
        "start": int(timestamp.timestamp()),
        "end": int((timestamp + timedelta(hours=1)).timestamp()),
        "appid": API_KEY
    }
    r = requests.get(url, params=params)
    if r.status_code == 200 and r.json().get("list"):
        return r.json()["list"][0]
    else:
        print(f"Weather API error {r.status_code} at {timestamp}")
        return None

def fetch_pollution_at(timestamp):
    url = "https://api.openweathermap.org/data/2.5/air_pollution/history"
    params = {
        "lat": LAT,
        "lon": LON,
        "start": int(timestamp.timestamp()),
        "end": int((timestamp + timedelta(hours=1)).timestamp()),
        "appid": API_KEY
    }
    r = requests.get(url, params=params)
    if r.status_code == 200 and r.json().get("list"):
        return r.json()["list"][0]
    else:
        print(f"⚠ Pollution API error {r.status_code} at {timestamp}")
        return None

def build_record(timestamp):
    weather = fetch_weather_at(timestamp)
    pollution = fetch_pollution_at(timestamp)
    time.sleep(1)  # avoid rate limits
    if not weather or not pollution:
        return None
    comp = pollution["components"]
    return {
        "timestamp": timestamp.strftime("%Y-%m-%d %H:%M:%S"),
        "aqi": pollution["main"]["aqi"],
        "pm2_5": comp["pm2_5"],
        "pm10": comp["pm10"],
        "temperature": round(weather["main"]["temp"] - 273.15, 2),
        "humidity": weather["main"]["humidity"],
        "wind_speed": weather["wind"]["speed"]
    }

# ================= SAVE FUNCTION =================
def save_records(records):
    df = pd.DataFrame(records)
    if os.path.exists(CSV_PATH):
        old = pd.read_csv(CSV_PATH)
        df = pd.concat([old, df], ignore_index=True)
        df.drop_duplicates(subset=["timestamp"], inplace=True)
    df.to_csv(CSV_PATH, index=False)
    print(f"Saved {len(df)} total records to {CSV_PATH}")

# ================= MAIN FUNCTION =================
def collect_data_5h(start_date, end_date):
    current = start_date
    records = []
    while current < end_date:
        print(f"Fetching {current}")
        record = build_record(current)
        if record:
            records.append(record)
        current += timedelta(hours=5)
    save_records(records)

# ================= RUN DATA COLLECTION =================
if __name__ == "__main__":
    start = datetime.now() - timedelta(days=365)  # 1 year ago
    end = datetime.now()
    collect_data_5h(start, end)

# ================= LOAD DATA =================
import hopsworks
os.environ["HOPSWORKS_API_KEY"] = "lfFwLUOhGfLXvEhH.YMimJy6BGMCztKnmFuq5MCZ1A79URh51YM8amIQJSUtOMw8R7p7YYN3cVUxErhZX"

df = pd.read_csv(CSV_PATH)
df["timestamp"] = pd.to_datetime(df["timestamp"])

# ================= HANDLE OUTLIERS =================
numeric_cols = ["temperature", "humidity", "wind_speed", "pm2_5", "pm10", "aqi"]

def cap_outliers(df, cols):
    capped_df = df.copy()
    for col in cols:
        Q1 = capped_df[col].quantile(0.25)
        Q3 = capped_df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        capped_df[col] = np.where(
            capped_df[col] < lower, lower,
            np.where(capped_df[col] > upper, upper, capped_df[col])
        )
    return capped_df

df_capped = cap_outliers(df, numeric_cols)
df = df_capped.copy()

# ================= PREPARE DATA FOR MULTI-OUTPUT FORECAST =================
def prepare_multioutput_forecast_data(df, lag_steps=15, forecast_steps=15):
    if len(df) < lag_steps + forecast_steps:
        print(f"Not enough rows. Need at least {lag_steps + forecast_steps}, got {len(df)}.")
        return None, None

    # Lag features
    lag_df = pd.concat([df["aqi"].shift(i) for i in range(1, lag_steps + 1)], axis=1)
    lag_df.columns = [f"aqi_lag_{i}" for i in range(1, lag_steps + 1)]

    # Targets
    target_df = pd.concat([df["aqi"].shift(-i) for i in range(1, forecast_steps + 1)], axis=1)
    target_df.columns = [f"target_t_plus_{i}" for i in range(1, forecast_steps + 1)]

    # Combine and clean
    final_df = pd.concat([lag_df, target_df], axis=1)
    final_df.dropna(inplace=True)

    X = final_df[lag_df.columns]
    y = final_df[target_df.columns]
    return X, y

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

X, y = prepare_multioutput_forecast_data(df)
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)

# ================= TRAIN RANDOM FOREST =================
from sklearn.multioutput import MultiOutputRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

rf_model = MultiOutputRegressor(RandomForestRegressor())
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)

rf_mae = mean_absolute_error(y_test, rf_pred)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))
rf_r2 = r2_score(y_test, rf_pred)
rf_acc = max(0, 1 - (rf_mae / np.mean(y_test)))

print("Random Forest Performance:")
print("MAE:", round(rf_mae, 2))
print("RMSE:", round(rf_rmse, 2))
print("R²:", round(rf_r2, 4))
print("Accuracy:", round(rf_acc * 100, 2), "%")

# ================= FORECAST NEXT 3 DAYS =================
latest_lags = df["aqi"].tail(15).values.reshape(1, -1)
latest_lags_scaled = scaler.transform(latest_lags)
forecast = rf_model.predict(latest_lags_scaled)[0]

day1 = forecast[:3]  # 3 intervals per day (~5-hour steps)
day2 = forecast[3:6]
day3 = forecast[6:9]

print("Day 1 Forecast:", day1)
print("Day 2 Forecast:", day2)
print("Day 3 Forecast:", day3)

# ================= SAVE MODEL TO HOPSWORKS =================
import joblib
from hsml.schema import Schema

# Save locally
joblib.dump(rf_model, "rf_model.pkl", compress=3)

# Create schema
model_schema = Schema(X)

# Hopsworks login & model registry
project = hopsworks.login()
mr = project.get_model_registry()

model = mr.python.create_model(
    name="karachi_aqi_forecaster_5h",
    metrics={"accuracy": round(rf_acc, 4)},
    model_schema=model_schema,
    input_example=X.iloc[0],
    description="Random Forest model for 3-day AQI forecast every 5 hours"
)

model.save("rf_model.pkl")
print(f"Model saved to Hopsworks with accuracy: {round(rf_acc * 100, 2)}%")